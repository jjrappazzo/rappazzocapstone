---
title: United States Car Accident Project
author: "Jason Rappazzo"
output:
    quarto::html_document:
      self_contained: false
    includes:
      in_header: shiny.html
runtime: shiny
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc-location: left
    number-sections: true
---

```{r setup, include = F}
library(tidyverse)
library(skimr)   # a better summary of data.frame
library(scales)  # scales for ggplot
library(ggthemes)  # additional ggplot themes
theme_set(theme_minimal()) # setting the minimal theme for ggplot
# setting default chunk options
knitr::opts_chunk$set(echo = T, eval = T,
                      message=F, warning = F) 
```

# Indroduction

## Research Questions

## Overview of Modeling Techniques

<u> Linear Regression:</u> also known as Ordinary Least Squares (OLS) is the most simple method for regression analysis.

The linear regression model for predicting the severity of a car accident is:

$$\hat{y}=\hat{\beta_0} + \hat{\beta_i} \times x + \epsilon$$

where $\hat{y}$ is the predicted severity of a car accident, $\hat{\beta_0}$ is the intercept term, $\hat{\beta_i}$ is the beta estimate for each feature, and $\epsilon$ is the error term.

The model minimizes the mean squared error, which is written as:

$$MSE({\beta})=\frac{1}{n} \sum_{i=1}^n  \hat{\epsilon_i}^2=\frac{1}{n}\sum_{i=1}^n  (y_i-\hat{y_i})^2$$

where $\hat{y_i}$ is the predicted severity of a car accident and $y_i$ is the actual severity.


<u> Ridge Regression: </u> A regularized version of linear regression. This is achieved by adding this equation to the loss function:

$$\sum_{i=1}^n  \beta_i^2$$

The cost function for ridge regression is as follows:

$$J(\beta) = MSE(\beta) + \frac{1}{n} \alpha \sum_{i=1}^n  \beta_i^2$$

where $MSE(\beta)$ is the mean squared error function, $\beta_i$ is the coefficient of the $i$-th feature, $n$ is the number of samples, and $\alpha$ is the regularization strength.

<u> Lasso Regression: </u> Like Ridge Regression, Lasso Regression adds a regularization term to the cost function, but uses the l1 norm of the weight vector instead of half the square of the l2 norm. The cost function for Lasso Regression is as follows:

$$J(\beta) = MSE(\beta) +  \alpha \sum_{i=1}^n  |\beta_i|$$

where $MSE(\beta)$ is the mean squared error function, $\beta_i$ is the coefficient of the $i$-th feature, $n$ is the number of samples, and $\alpha$ is the regularization strength.


<u> Random Forest: </u> They are comprised of many slightly different decision trees. The decision trees that make up the random forest are generated randomly giving it the name Random Forest. Depending on the number of trees in the forest, this could be a highly accurate model, but could also have the tendency to over fit the data. Random Forests are one way to combat the over fitting problem that Decision Tree Models may have. 


# Raw Data

```{r, include=FALSE}
library(tidyverse)
# library(readr)


# I am using a 10% sample of the original data set with 2.8 million observations
path <- "C:/Users/jjrap/OneDrive/Desktop/sample_accidents.csv"
accident_raw <- read_csv(path)


accident_raw <- accident_raw %>% 
  na.omit()
```

```{r, echo=FALSE,collapse=TRUE, warning=FALSE, message=FALSE}
rmarkdown::paged_table(accident_raw)
```
## About the Data

## Data Description

## Data Visualization
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
ggplot(accident_raw, aes(x = Severity)) +
  geom_bar(fill = "darkgreen") +
  labs(title = "Number of Accidents by Severity", x = "Severity", y = "Count")+ 
  theme_minimal()
```
Most of the car accidents recorded had a severity score of 2 out of the highest posible 4. The data is not balanced.

<br>


```{r, collapse=TRUE, warning=FALSE, message=FALSE,fig.height=8, fig.width= 10}
library(dplyr)

accident_freq <- accident_raw %>% 
  count(State) %>% 
  arrange(desc(n))

ggplot(accident_freq, aes(x = reorder(State, -n), y = n)) +
  geom_bar(fill = "darkgreen", stat = "identity") +
  labs(title = "Number of Accidents by State", x = "State", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+
  ylab("Count of Accidents")+ xlab("")+
  theme_minimal()
```
The highest amount of car accidents in this particular data set came from Florida and California each with over 20,000 separate accidents.

<br>

```{r, collapse=TRUE, warning=FALSE, message=FALSE,fig.height=8, fig.width= 10}
library(shiny)
library(plotly)
library(dplyr)

ui <- fluidPage(
  selectInput("severity", "Severity Level:",
              choices = c(1, 2, 3, 4), selected = 1),
  plotlyOutput("plot")
)

server <- function(input, output) {
  
  data <- reactive({
    accident_raw %>%
      filter(Severity == input$severity) %>%
      group_by(State) %>%
      summarise(n = n()) %>% 
      arrange(desc(n))
  })
  
  output$plot <- renderPlotly({
    plot_ly(data(), x = ~State, y = ~n, type = 'bar') %>%
      layout(xaxis = list(title = "State"),
             yaxis = list(title = "Frequency of Accidents"))
  })
  
}

shinyApp(ui = ui, server = server)


```


```{r, collapse=TRUE, warning=FALSE, message=FALSE,fig.height=8, fig.width= 10}
library(tidytext)
accident_raw %>%
  group_by(Severity) %>%
  count(Weather_Condition) %>%
  mutate(n = n / sum(n)) %>%
  top_n(10, n) %>%
  ggplot(aes(reorder_within(Weather_Condition, n, Severity), n)) +
  geom_col(aes(fill = !Weather_Condition == "Clear"), show.legend = F) +
  facet_wrap(~ Severity, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  scale_y_continuous(breaks = seq(0, 0.6, 0.05), labels = percent) +
  labs(x = "Weather Condition",
       y = "Proportion",
       title = "Proportion of Top 10 Weather Conditions for Each Severity Level")
```
This graph shows that weather condition does not have that much of an impact on severity level. Each of the severity levels showed very high proportions of fair weather. 

<br>

# Preparing Data For Machine Learning
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
accident_12var <- accident_raw %>% 
  select(Severity,State, `Temperature(F)`, `Humidity(%)`,
         `Visibility(mi)`, `Wind_Speed(mph)`, Weather_Condition,
         `Precipitation(in)`, Crossing, Junction, Traffic_Signal,
         Sunrise_Sunset)

colnames(accident_12var) <- gsub("\\)|\\%|\\(", ".", colnames(accident_12var))
```

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
library(caret)
library(recipes)
library(dplyr)

# Split the data into training and testing sets
set.seed(2)
train_indices <- createDataPartition(accident_12var$Severity, p = 0.8, list = FALSE)
train_set <- accident_12var[train_indices, ]
test_set <- accident_12var[-train_indices, ]


# TRAIN SET

# Make a copy of the train set
copied_traindata <- data.frame(train_set)

# Add an id column to copied_traindata
copied_traindata <- copied_traindata %>% mutate(id = row_number())

# Separate Label from Feature
accident <- select(copied_traindata, -Severity) # drop Severity column
label <- copied_traindata$Severity # select Severity column

# Separate Numerical from Categorical
accident_num <- accident %>% 
  select(id, Temperature.F., Humidity..., Visibility.mi., Wind_Speed.mph., Precipitation.in.)

accident_cat <- accident %>% 
  select(id, State, Weather_Condition, Crossing, Junction, Traffic_Signal, Sunrise_Sunset)

# Define numeric and categorical attributes
num_attribs <- names(accident_num)[2:6]
cat_attribs <- names(accident_cat)[2:7]

# Define preprocessing pipelines
num_pipeline <- recipe(~., data = accident_num) %>%
  step_impute_median(all_numeric(), -has_role("id")) %>%
  step_center(all_numeric(), -has_role("id")) %>%
  step_scale(all_numeric(), -has_role("id"))

cat_pipeline <- recipe(~., data = accident_cat) %>%
  step_dummy(all_nominal())

# Merge the preprocessed numerical and categorical features into a single dataset

accident <- accident %>% rename(Index = id) 

df1 <- mutate(num_pipeline %>% prep() %>% bake(new_data = NULL), join_key = "Index")
df2 <- mutate(cat_pipeline %>% prep() %>% bake(new_data = NULL), join_key = "Index")

accident_prepared <- accident %>% 
  select(-one_of(c(cat_attribs, num_attribs)))

accident_prepared <- cbind(accident_prepared, df1,df2)



accident_prepared <- accident_prepared %>% 
  distinct()

accident_prepared <- select(accident_prepared, -c("Index", "id", "join_key", "id.1", "join_key.1"))






#TEST SET
# Make a copy of the test set
copied_testdata <- data.frame(test_set)

# Add an id column to copied_testdata
copied_testdata <- copied_testdata %>% mutate(id = row_number())

# Separate Label from Feature
accident_test <- select(copied_testdata, -Severity) # drop Severity column
label_test <- copied_testdata$Severity # select Severity column

# Separate Numerical from Categorical
accident_num_test <- copied_testdata %>% 
  select(Temperature.F., Humidity..., Visibility.mi., Wind_Speed.mph., Precipitation.in.)

accident_cat_test <- copied_testdata %>% 
  select(State, Weather_Condition, Crossing, Junction, Traffic_Signal, Sunrise_Sunset)

# Define numeric and categorical attributes
num_attribs <- names(accident_num_test)[1:6]
cat_attribs <- names(accident_cat_test)[1:7]

# Define preprocessing pipelines
num_pipeline <- recipe(~., data = accident_num_test) %>%
  step_impute_median(all_numeric(), -has_role("id")) %>%
  step_center(all_numeric(), -has_role("id")) %>%
  step_scale(all_numeric(), -has_role("id"))

cat_pipeline <- recipe(~., data = accident_cat_test) %>%
  step_dummy(all_nominal())

# Merge the preprocessed numerical and categorical features into a single dataset

copied_testdata <- copied_testdata %>% rename(Index = id) 

df1 <- mutate(num_pipeline %>% prep() %>% bake(new_data = NULL), join_key = "Index")
df2 <- mutate(cat_pipeline %>% prep() %>% bake(new_data = NULL), join_key = "Index")

accident_prepared_test <- accident_test %>% 
  select(-one_of(c(cat_attribs, num_attribs)))

accident_prepared_test <- cbind(accident_prepared_test, df1,df2)



accident_prepared_test <- accident_prepared_test %>% 
  distinct()

accident_prepared_test <- select(accident_prepared_test, -c("id", "join_key", "join_key.1"))

```

```{r, include = FALSE, warning=FALSE, echo=FALSE}
accident_prepared_test$Weather_Condition_Blowing.Dust...Windy <- 0
accident_prepared_test$Weather_Condition_Clear <- 0
accident_prepared_test$Weather_Condition_Blowing.Snow...Windy <- 0
accident_prepared_test$Weather_Condition_Freezing.Drizzle <- 0
accident_prepared_test$Weather_Condition_Heavy.Drizzle <- 0
accident_prepared_test$Weather_Condition_Heavy.Sleet <- 0
accident_prepared_test$Weather_Condition_Heavy.Snow...Windy <- 0
accident_prepared_test$Weather_Condition_Light.Freezing.Rain...Windy <- 0
accident_prepared_test$Weather_Condition_Light.Ice.Pellets <- 0
accident_prepared_test$Weather_Condition_Light.Thunderstorms.and.Rain <- 0
accident_prepared_test$Weather_Condition_Sleet <- 0
accident_prepared_test$Weather_Condition_Thunder...Wintry.Mix <- 0
accident_prepared_test$Weather_Condition_Smoke...Windy <- 0
accident_prepared_test$Weather_Condition_Thunder.and.Hail <- 0
accident_prepared_test$Weather_Condition_Widespread.Dust <- 0
accident_prepared_test$Weather_Condition_Widespread.Dust...Windy <- 0
accident_prepared_test$Weather_Condition_Light.Snow.Shower <- 0
```




# Models 

## Linear Regression

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
# Fit the linear regression model
lin_reg <- lm(label ~ ., data = accident_prepared)

# Use the model to predict the response variable using the test data
y_pred <- predict(lin_reg, newdata = accident_prepared_test)

# Calculate the residuals
residuals <- y_pred - label_test

# Calculate the squared errors
squared_errors <- residuals^2

# Calculate the mean squared error
mse <- mean(squared_errors)

# Print the MSE
cat("MSE:", mse)


```

## Ridge Regression

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
#ridge regression

library(glmnet)

# Separate the predictor variables from the response variable
y <- label
X <- as.matrix(select(accident_prepared, -label))

# Define the lambda sequence for ridge regression
lambda_seq <- 10^seq(10, -2, length = 100)

# Perform cross-validated ridge regression
ridge_fit <- cv.glmnet(X, y, alpha = 0, lambda = lambda_seq)

# Plot the cross-validation results
plot(ridge_fit)

ridge_coef <- coef(ridge_fit)[-1]

y_pred <- predict(ridge_fit, newx = X)

mse <- mean((y - y_pred)^2)


# Print the MSE
cat("MSE:", mse)



```

## Lasso Regression
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
x <- model.matrix(~ ., data = accident_prepared) 
y <- label

# Fit a Lasso regression with cross-validation
lasso_model <- cv.glmnet(x, y, alpha = 1) 

extra_columns <- setdiff(colnames(accident_prepared_test), colnames(accident_prepared))

accident_prepared_test <- accident_prepared_test %>%
                          select(-one_of(extra_columns))


# Predict the response variable using the test data
x_test <- model.matrix(~ ., data = accident_prepared_test) 
y_pred <- predict(lasso_model, newx = x_test)

# Calculate the MSE
mse <- mean((y_pred - label_test)^2)

# Print the MSE
cat("MSE:", mse)


plot(lasso_model)


```

## Random Forest
```{r}
library(randomForest)

# Train the model
accident_forest <- randomForest(label ~ ., data = accident_prepared, ntree = 10, mtry = 5)

# Make predictions on the test set
pred <- predict(accident_forest, newdata = accident_prepared_test)

# Generate confusion matrix
confusion_matrix <- table(pred, label_test)

# Calculate MSE
mse <- mean((label_test - pred)^2)


print(paste("Mean Squared Error:", mse))

```


# Results

## Mean Squared Error of all Models


## Linear Regression Results
```{r, collapse=TRUE, warning=FALSE, message=FALSE,echo=FALSE}
library(stargazer)

stargazer(lin_reg, type = "text", title = "Linear Regression Results", 
          ci = TRUE, ci.level = 0.90, single.row = TRUE,
          dep.var.caption = "Severity of Car Accident",
          omit.stat = "all")
          
```

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
library(coefplot)
library(broom)


# Extract coefficients and standard errors
coef_df <- tidy(lin_reg, conf.int = TRUE)

# Filter out intercept
coef_df <- coef_df[-1,]

num_coef_df <- coef_df[coef_df$term %in% num_attribs,]
cat_coef_df <- coef_df[grep(".*\\_.*", coef_df$term), ]

# Create plots
plot_num <- ggplot(num_coef_df, aes(x = estimate, y = reorder(term, estimate))) +
  geom_point(size = 2) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  labs(x = "Coefficient Estimate", y = "Variable") +
  ggtitle("Linear Regression Results for Numeric Variables") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  geom_vline(xintercept = 0, linetype = "dashed", color = "red")

plot_num


cat_coef_df1 <- cat_coef_df[1:25,]
cat_coef_df2 <- cat_coef_df[25:50,]
cat_coef_df3 <- cat_coef_df[50:75,]
cat_coef_df4 <- cat_coef_df[75:100,]
cat_coef_df5 <- cat_coef_df[100:125,]



# Create  separate plots
plot_cat1 <- ggplot(cat_coef_df1, aes(x = estimate, y = reorder(term, estimate))) +
  geom_point(size = 2) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  labs(x = "Coefficient Estimate", y = "Variable") +
  ggtitle("Linear Regression Results for Categorical Variables") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  geom_vline(xintercept = 0, linetype = "dashed", color = "red")

plot_cat1

plot_cat2 <- ggplot(cat_coef_df2, aes(x = estimate, y = reorder(term, estimate))) +
  geom_point(size = 2) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  labs(x = "Coefficient Estimate", y = "Variable") +
  ggtitle("Linear Regression Results for Categorical Variables") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  geom_vline(xintercept = 0, linetype = "dashed", color = "red")

plot_cat2

plot_cat3 <- ggplot(cat_coef_df3, aes(x = estimate, y = reorder(term, estimate))) +
  geom_point(size = 2) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  labs(x = "Coefficient Estimate", y = "Variable") +
  ggtitle("Linear Regression Results for Categorical Variables") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  geom_vline(xintercept = 0, linetype = "dashed", color = "red")
plot_cat3

plot_cat4 <- ggplot(cat_coef_df4, aes(x = estimate, y = reorder(term, estimate))) +
  geom_point(size = 2) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  labs(x = "Coefficient Estimate", y = "Variable") +
  ggtitle("Linear Regression Results for Categorical Variables") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  geom_vline(xintercept = 0, linetype = "dashed", color = "red")

plot_cat4

plot_cat5 <- ggplot(cat_coef_df5, aes(x = estimate, y = reorder(term, estimate))) +
  geom_point(size = 2) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  labs(x = "Coefficient Estimate", y = "Variable") +
  ggtitle("Linear Regression Results for Categorical Variables") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  geom_vline(xintercept = 0, linetype = "dashed", color = "red")

plot_cat5


```

## Decision Tree Results
```{r, collapse=TRUE, warning=FALSE, message=FALSE,fig.height=8, fig.width= 10}
varImpPlot(accident_forest, main = "Variable Importance Plot")
```






# Discussion

# Conclusion
